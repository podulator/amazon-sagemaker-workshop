## Predicting Customer Churn


In this module, we'll work our way through an example Jupyter notebook that demonstrates how to use a built-in algorithm in SageMaker. More specifically, we'll use Amazon SageMaker's version of XGBoost, a popular and efficient open-source implementation of the gradient boosted trees algorithm. 

Gradient boosting is a supervised learning algorithm that attempts to predict a target variable by combining the estimates of a set of simpler, weaker models. XGBoost has done remarkably well in machine learning competitions because it robustly handles a wide variety of data types, relationships, and distributions. It often is a useful, go-to algorithm in working with structured data, such as data that might be found in relational databases and flat files. 

Follow these steps:

1. Go to your Jupyter notebooks homepage 

2. Click into the folder called `amazon-sagemaker-workshop`

3. Click into the folder called `notebooks`

4. Click on the notebook called `customer-churn.ipynb`, then follow the directions in the notebook.

<p><strong>NOTE:  training the model for this example typically takes about 5 minutes or less.</strong></p>

[**Return to the workshop**](../Workshop1/README.md)