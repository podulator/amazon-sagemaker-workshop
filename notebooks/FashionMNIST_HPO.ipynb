{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Linear Learner algorithm\n",
    "\n",
    "#### Predicting fashion type using Zalando's Fasion-MNIST dataset (https://github.com/zalandoresearch/fashion-mnist), Amazon SageMaker Linear Learner algorithm for classification and Amazon SageMaker Automatic Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright [2018]-[2018] Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at - \n",
    "\n",
    "http://aws.amazon.com/apache2.0/\n",
    "\n",
    "or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)\n",
    "2. [Prerequisites and Preprocessing](#Prequisites-and-Preprocessing)\n",
    "  1. [Permissions and environment variables](#Permissions-and-environment-variables)\n",
    "  2. [Data ingestion](#Data-ingestion)\n",
    "  3. [Data inspection](#Data-inspection)\n",
    "  4. [Data conversion](#Data-conversion)\n",
    "  5. [Upload training data](#Upload-training-data)\n",
    "3. [Training the linear model](#Training-the-linear-model)\n",
    "4. [Set up hosting for the model](#Set-up-hosting-for-the-model)\n",
    "5. [Test the model and see how it performs](#Test-the-model-and-see-how-it-performs)\n",
    "6. [Improving the mode](#Improving-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to our example introducing Amazon SageMaker! To drive this example, we will show you how to use Amazon SageMaker to learn how to build and deploy a model to recognize the type of apparel from images.\n",
    "\n",
    "The first step in machine learning is to get data to learn from. For our example, we will use the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which consists of images of apparel, classified from zero to nine representing what kind of apparel they represent.\n",
    "\n",
    "This dataset contains labeled examples ( **`x`**, `y`) where **`x`** is a high dimensional vector and `y` is a numeric label. As the dataset we are using for training contains labels for what the correct answer \"should be\", this example falls under the category of Supervised Learning. Once the model is deployed, we will be predicting which of 10 possible values (0,1,2 .. 9) the image falls under, the type of machine learning we are doing is Supervised Classification. Lastly, since the predictions can be one of 10 values rather than a yes/no type of response, this example demonstrates Supervised Multiclass Classification.\n",
    "\n",
    "To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prequisites and Preprocessing\n",
    "\n",
    "### Permissions and environment variables\n",
    "\n",
    "We use Amazon Simple Storage Service (Amazon S3) to store the training and model data. The S3 bucket should be within the same region as the Notebook Instance, training, and hosting.\n",
    "\n",
    "If you haven't create an S3 bucket for the use with SageMaker in your account yet you can create one now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker\n",
    "\n",
    "import sagemaker\n",
    "from datetime import datetime\n",
    "\n",
    "# customize environment name to help you find your training jobs and endpoints in SageMaker console\n",
    "envname = 'DH-test'\n",
    "\n",
    "# --- do not change values below ---\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "timestamp = datetime.utcnow().strftime(\"%s\")\n",
    "print('timestamp :: {}'.format(timestamp))\n",
    "basicModelName = 'linear-model-basic-{}-{}'.format(envname, timestamp)\n",
    "advancedModelName = 'linear-model-adv-{}-{}'.format(envname, timestamp)\n",
    "basicJobName = 'linear-job-basic-{}-{}'.format(envname, timestamp)[:32]\n",
    "advancedJobName = 'linear-job-adv-{}-{}'.format(envname, timestamp)[:32]\n",
    "\n",
    "# the prefix defines the location of our data for this excercise\n",
    "prefix = 'sagemaker/{}'.format(envname)\n",
    "# the IAM role is granting SageMaker access to the required resources for this excercise\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next we download the dataset and apply some preprocessing prior to training. In this examples this is done using the Jupyter Notebook. For large datasets this would be done *in situ* by Amazon Glue, Apache Spark in Amazon EMR, etc.\n",
    "\n",
    "The dataset has been made publicly available by Zalando Research [here](https://github.com/zalandoresearch/fashion-mnist). The block below downloads the dataset and extracts the data according to the instructions provided in the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import zipfile, urllib.request, sys\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "urllib.request.urlretrieve(\"https://github.com/zalandoresearch/fashion-mnist/archive/master.zip\", \"fashion-mnist.zip\")\n",
    "zip_ref = zipfile.ZipFile(\"fashion-mnist.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "\n",
    "sys.path.append('./fashion-mnist-master/utils')\n",
    "\n",
    "import mnist_reader\n",
    "X_train_val, y_train_val = mnist_reader.load_mnist('fashion-mnist-master/data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('fashion-mnist-master/data/fashion', kind='t10k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is structured into two sets of data: One set with 60k labeled entries and another with 10k labeled entries. We will now split the first set into 50k entries for training & 10k entries for validation. The second data set (also 10k) will be used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into training and validation sets\n",
    "X_validation = X_train_val[0:10000]\n",
    "y_validation = y_train_val[0:10000]\n",
    "X_train = X_train_val[10000:60000]\n",
    "y_train = y_train_val[10000:60000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is loaded, it is typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook.\n",
    "Note that upon loading the data, train_set contain the n'th image in test_set[0][n] and the corresponding label (which apparel type it is supposed to be) in test_set[1][n]. \n",
    "\n",
    "As an example, let's go ahead and look at the 30th image that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (2,10)\n",
    "\n",
    "\n",
    "def show_item(img, caption='', subplot=None):\n",
    "    if subplot==None:\n",
    "        _,(subplot)=plt.subplots(1,1)\n",
    "    imgr=img.reshape((28,28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "    \n",
    "\n",
    "apparel_lookup = {0: 'T-shirt/top',\n",
    "                  1: 'Trouser',\n",
    "                  2: 'Pullover',\n",
    "                  3: 'Dress',\n",
    "                  4: 'Coat',\n",
    "                  5: 'Sandal',\n",
    "                  6: 'Shirt',\n",
    "                  7: 'Sneaker',\n",
    "                  8: 'Bag',\n",
    "                  9: 'Ankle boot'}\n",
    "\n",
    "\n",
    "show_item(X_train[30], 'This is a {}'.format(apparel_lookup[y_train[30]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing an Algorithm\n",
    "\n",
    "As we are looking to perform Supervised Multiclass Classification, let's find an appropriate algorithm from among the [Amazon SageMaker built-in algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) makes available to you.\n",
    "\n",
    "Looking through the list, it looks like the Linear Learner, XGBoost Algorithm or the Image Classification Algorithm would fit  to our needs. \n",
    "\n",
    "If you have a highly specialized use case, you can also [bring your own algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html)\n",
    "\n",
    "For this example, we will pick the Linear Learner to get familiar with SageMaker and its capabilities. We encourage you to explore the other algorithms [here (XGBoost for MNIST)](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/xgboost_mnist) and [here (Image Classification Algorithm for Fashion MNIST)](https://aws.amazon.com/blogs/machine-learning/classify-your-own-images-using-amazon-sagemaker/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation of Linear Learner takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk.\n",
    "\n",
    "Most of the conversion effort is handled by the Amazon SageMaker Python SDK, imported as `sagemaker` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "# Write the training data\n",
    "vectors = np.array([t.tolist() for t in X_train]).astype('float32')\n",
    "labels = np.array([t.tolist() for t in y_train]).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload training data\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our inputs are in S3, ready to be used for training, let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear model\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. Since this data is relatively small, it isn't meant to show off the performance of the Linear Learner training algorithm, although we have tested it on multi-terabyte datasets.\n",
    "\n",
    "Again, we'll use the Amazon SageMaker Python SDK to kick off training, and monitor status until it is completed.  In this example that takes between 7 and 11 minutes.  Despite the dataset being small, provisioning hardware and loading the algorithm container take time upfront.\n",
    "\n",
    "First, let's specify our container.  Since we want this notebook to run in any of the Amazon SageMaker's regions, we'll create a small lookup.  More details on algorithm containers can be found in [AWS documentation](https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html). Note that the lookup also gets to location of the Linear Learner algorithm in the region that the notebook is running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner')\n",
    "print('using container image: {}'.format(container))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the container we have obtained, we will create a sagemaker estimator object that represents the Linear Learner algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess,\n",
    "                                       tags=[ \n",
    "                                           {'Key':'model', 'Value':'linear-learner'}, \n",
    "                                           {'Key':'version', 'Value':'basic'}, \n",
    "                                           {'Key': 'Name', 'Value':basicModelName}\n",
    "                                       ], \n",
    "                                       base_job_name='{}-trainjob'.format(envname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to training on the dataset, we have to specify some hyperparameters. Hyperparameters can be seen as information that we provide to the estimator to control the way the estimator treats and learns from the data.\n",
    "\n",
    "- `feature_dim` is set to 784, which is the number of pixels in each 28 x 28 image.\n",
    "- `predictor_type` is set to 'multi_classifier' since we are doing a Multiclass Classification\n",
    "- `num_classes` is set to 10 since we can have 10 different predictions for an image (0,1,2...9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.set_hyperparameters(feature_dim=784,\n",
    "                           predictor_type='multiclass_classifier',\n",
    "                           num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our model. We can do this simply by calling fit() and specificying our training and validation data. While you wait for the training to finish, take some time to review the [AWS Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) to find out what other hypermarameter options are available for the Linear Learner algorithm or to get a feeling of the other algorithms available ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.fit(\n",
    "                {'train': s3_train_data}, \n",
    "                job_name=basicJobName\n",
    "          )\n",
    "jobStatus = boto3.client('sagemaker').describe_training_job(TrainingJobName=basicJobName)\n",
    "print(jobStatus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint.  This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "_Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, or other deployment target, we will discuss more of those options towards the end of the workshop._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge', \n",
    "                                 endpoint_name='{}-endpoint'.format(envname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a supported region, like us-west-1, this could be changed to something like\n",
    "```\n",
    "linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.c5.large',\n",
    "                                 accelerator_type='ml.eia1.medium',\n",
    "                                 endpoint_name='{}-endpoint'.format(envname))\n",
    "```\n",
    "And that is all you need to do to use Elastic Inference. \n",
    "\n",
    "ml.c5.large costs $0.134/hour in us-west-1\n",
    "ml.eial.medium costs $0.140/hour in us-west-1\n",
    "vs. an \n",
    "ml.p3.2xlarge, which is $4.284/hour in us-west-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model and see how it performs\n",
    "Finally, we can now validate the model for use.  We can pass HTTP POST requests to the endpoint to get back predictions.  To make this easier, we'll again use the Amazon SageMaker Python SDK and specify how to serialize requests and deserialize responses that are specific to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try getting a prediction for a single record. We will display the image and the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 45\n",
    "result = linear_predictor.predict(X_test[ind:ind+1])\n",
    "show_item(X_test[ind:ind+1], 'This is a {}'.format(apparel_lookup[int(float(result['predictions'][0]['predicted_label']))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, a single prediction seems to work.  We see that for one record our endpoint returned some JSON which contains `predictions`, including the `score` and `predicted_label`.  In this case, `score` will be a continuous value between [0, 1] representing the probability we think that the image corresponds to each possible prediction (0,1,2,.9).  `predicted_label` will be a value from (0,1,2,3...9) which indicates what number the model thinks the image contains\n",
    "\n",
    "Let's do a whole batch of images and evaluate our predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(X_test, 100):\n",
    "    result = linear_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we that we have our predictions from the test set, let us build a confusion matrix. The confusion matrix shows us the count of the actual labels (along the rows) versus the predicted labels (along the columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "actual_labels = np.array(y_test)\n",
    "pd.crosstab(actual_labels, predictions, rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this matrix is dominant along the diagonal, it looks like the algorithm did pretty well! Let us check how often we got it right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_preds = 0\n",
    "for i in range(1,len(actual_labels)):\n",
    "    if actual_labels[i] == predictions[i]:\n",
    "        correct_preds = correct_preds + 1\n",
    "\n",
    "print('The model predicted correctly on {}% of the test set'.format(correct_preds*100/len(actual_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a fun next step, let us try to classify a random piece of apparel that you get off the internet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Change the imgsrc with a link to an image of your choice with a white background\n",
    "imgsrc = 'https://images-na.ssl-images-amazon.com/images/I/81OaXwn1x4L._UX679_.jpg'\n",
    "#imgsrc = 'https://images-eu.ssl-images-amazon.com/images/I/31TcgNHsbIL._AC_UL260_SR200,260_.jpg'\n",
    "#imgsrc = 'https://images-eu.ssl-images-amazon.com/images/I/41hWhZBIc3L._AC_UL260_SR200,260_.jpg'\n",
    "imglocname = 'localimage.jpg'\n",
    "\n",
    "#Download the file locally\n",
    "urllib.request.urlretrieve(imgsrc, imglocname)\n",
    "\n",
    "# Resize and convert to grayscale\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot((255-rgb[...,:3]), [0.299, 0.587, 0.114])\n",
    "\n",
    "fullimgread = Image.open(imglocname)   \n",
    "imgread = fullimgread.resize((28, 28))\n",
    "gray = rgb2gray(np.asarray( imgread, dtype=\"int32\" ))\n",
    "\n",
    "# Do the prediction!\n",
    "result = linear_predictor.predict(gray.reshape((1,784)))\n",
    "\n",
    "print ('This is a {}'.format(apparel_lookup[int(float(result['predictions'][0]['predicted_label']))]))\n",
    "plt.imshow(fullimgread)\n",
    "plt.axis('off')\n",
    "plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "\n",
    "Despite having a good model, let us take a moment to see if we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "We discussed hyperparameters earlier that we used to instruct the algorithm on some characteristics of the data. However, hyperparameters also include some adjustable 'settings' for the algorithm itself. So far, we have used the default values, but picking the right values for some of these hyperparameters can help us improve the model.\n",
    "\n",
    "Each algorithm has its own hyperparameters. If you have not managed to look them up by yourself earlier, these are the [hyperparameters for the linear learner](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html)\n",
    "\n",
    "How can we arrive at the 'best values' for these hyperparameters? Amazon SageMaker provides [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html) to allow you to use your test and validation datasets to identify the best hyperparameters for your particular use case.\n",
    "\n",
    "First, let us prepare the validation data in the same way we prepared the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the validation data\n",
    "validation_vectors = np.array([t.tolist() for t in X_validation]).astype('float32')\n",
    "validation_labels = np.array([t.tolist() for t in y_validation]).astype('float32')\n",
    "\n",
    "validation_buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(validation_buf, validation_vectors, validation_labels)\n",
    "validation_buf.seek(0)\n",
    "\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', key)).upload_fileobj(validation_buf)\n",
    "s3_validation_data = 's3://{}/{}/validation/{}'.format(bucket, prefix, key)\n",
    "print('uploaded validation data location: {}'.format(s3_validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now set up Amazon SageMaker's hyperparameter tuning capabilities. We will use four of the [Linear Learner Tuning Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner-tuning.html) and their recommended ranges. \n",
    "\n",
    "Note: These were chosen primarily to demonstrate the different types of hyperparameters you can tune. You might be able to achieve better or faster results by choosing a different set of hyperparameters and ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "\n",
    "hyperparameters_to_tune = { 'wd': ContinuousParameter(0.001, 0.1),\n",
    "                         'learning_rate': ContinuousParameter(0.001, 0.01),\n",
    "                        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to choose a metric for the hyperparameter optimization. For this example, we will use the validation:objective_loss metric. This is also the reason we are including validation data in our model tuning step below.\n",
    "\n",
    "Let us set up the tuner itself and have it tune the model and hopefully come up with a better model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "linear_tuner = HyperparameterTuner(estimator=linear,\n",
    "                            objective_metric_name = 'validation:objective_loss',\n",
    "                            objective_type = 'Minimize',\n",
    "                            hyperparameter_ranges=hyperparameters_to_tune,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=2,\n",
    "                            tags=[ \n",
    "                                {'Key':'model', 'Value':'linear-learner'}, \n",
    "                                {'Key':'version', 'Value':'hpo'}, \n",
    "                                {'Key': 'Name', 'Value':advancedModelName}\n",
    "                            ], \n",
    "                            base_tuning_job_name='{}-hpojob'.format(envname))\n",
    "\n",
    "linear_tuner.fit(\n",
    "    inputs={'train': s3_train_data, 'validation': s3_validation_data}, \n",
    "    include_cls_metadata=False, \n",
    "    job_name=advancedJobName\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call fit() with a HyperparameterTuner object, it initiates a job that will typically run through many iterations. During this process, you can always access the best model the job has found so far. For our example today, we will wait until the job is complete. Run the block below to get the latest status. When the status is 'Completed', we will move to the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "sagemaker = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "tuning_job_result = sagemaker.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=advancedJobName)\n",
    "status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "print(status)\n",
    "counter = 0\n",
    "best_job_yet = None\n",
    "last_job_count = -1\n",
    "job_count = 0\n",
    "\n",
    "while status != 'Completed' and counter < 60:\n",
    "\n",
    "    if tuning_job_result.get('BestTrainingJob', None):\n",
    "        current_job = tuning_job_result['BestTrainingJob']\n",
    "        if current_job != best_job_yet:\n",
    "            best_job_yet = current_job\n",
    "            print(\"Best model found so far:\")\n",
    "            pprint(tuning_job_result['BestTrainingJob'])\n",
    "\n",
    "    job_count = tuning_job_result['TrainingJobStatusCounters']['Completed']\n",
    "    if job_count != last_job_count:\n",
    "        last_job_count = job_count\n",
    "        print(\"%d training jobs have completed out of 10\" % job_count)\n",
    "        statstr = 'Status: {}'.format(tuning_job_result['HyperParameterTuningJobStatus'])\n",
    "        if (tuning_job_result['HyperParameterTuningJobStatus'] != 'Completed'):\n",
    "            statstr = statstr + '\\nRunning Since: ' + str(tuning_job_result['CreationTime'])\n",
    "            statstr = statstr + '\\nTraining Jobs Completed: ' + str(tuning_job_result['TrainingJobStatusCounters']['Completed'])\n",
    "            statstr = statstr + '\\nTraining Jobs In Progress: ' + str(tuning_job_result['TrainingJobStatusCounters']['InProgress'])\n",
    "            fail_jobs = tuning_job_result['TrainingJobStatusCounters']['RetryableError'] + tuning_job_result['TrainingJobStatusCounters']['NonRetryableError']\n",
    "            statstr = statstr + '\\nTraining Jobs Failed: ' + str(fail_jobs)\n",
    "            statstr = statstr + '\\nMaximum Training Jobs: ' + str(tuning_job_result['HyperParameterTuningJobConfig']['ResourceLimits']['MaxNumberOfTrainingJobs'])\n",
    "        print(statstr)\n",
    "\n",
    "    # sleep for 30 and then update markers\n",
    "    time.sleep(30)\n",
    "    counter += 1\n",
    "    tuning_job_result = sagemaker.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=advancedJobName)\n",
    "    status = tuning_job_result['HyperParameterTuningJobStatus']\n",
    "    if status == 'Failed':\n",
    "        raise Exception('Job failed because :: {}'.format(tuning_job_result['FailureReason']))\n",
    "\n",
    "if tuning_job_result.get('BestTrainingJob', None):\n",
    "    print(\"Best model after training:\")\n",
    "    pprint(tuning_job_result['BestTrainingJob'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running a variety of training jobs, the hyperparameter tuner has now identified a set of hyperparameters that give us the best results in terms of the objective metric (in our case, the objective_loss). Let us go ahead and deploy this model in a similar fashion to how we deployed the model earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_tuned_predictor = linear_tuner.deploy(initial_instance_count=1,\n",
    "                                 instance_type='ml.m4.xlarge',endpoint_name='{}-tunedendpoint'.format(envname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the test set against our newly deployed endpoint and gather the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_tuned_predictor.content_type = 'text/csv'\n",
    "linear_tuned_predictor.serializer = csv_serializer\n",
    "linear_tuned_predictor.deserializer = json_deserializer\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(X_test, 100):\n",
    "    result = linear_tuned_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the moment of truth! Let us calculate how often the tuned model got the right prediction and also generate the corresponding confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = np.array(y_test)\n",
    "correct_preds = 0\n",
    "for i in range(1,len(actual_labels)):\n",
    "    if actual_labels[i] == predictions[i]:\n",
    "        correct_preds = correct_preds + 1\n",
    "\n",
    "print('The tuned model predicted correctly on {}% of the test set'.format(correct_preds*100/len(actual_labels)))\n",
    "\n",
    "\n",
    "pd.crosstab(actual_labels, predictions, rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "Let us quickly summarize what we achieved. We have taken a common dataset (Fashion MNIST) and used Amazon SageMaker with it's Linear Learner algorithm to train a multiclass classification model that allows us to predict fashion items among 10 different categories. Once trained we deployed the trained model using Amazon SageMaker's model hosting capabilities to create a fully-managed, automated scalable model endpoint. \n",
    "\n",
    "Finally we have used the Automatic Model Tuning to help us find the best version of a model by running many training jobs against our dataset using the algorithm and ranges of hyperparameters that we specified. \n",
    "\n",
    "### Clean up\n",
    "\n",
    "We can now delete the endpoints that we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete model endpoint\n",
    "linear_predictor.delete_endpoint()\n",
    "linear_tuned_predictor.delete_endpoint()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment strategies\n",
    "\n",
    "Ok, so we've trained a quick model, and an hyper parameter tuned model, deployed and tested them, and now torn down the endpoints.\n",
    "\n",
    "In a real world situation we would probably want to use an A/B deployment or similar, so let's have a look at that.\n",
    "\n",
    "Let's start off with defining a couple of helper functions that return a model ARN from a normal training job or the best model from an HPO job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def createModelFromHpoJob(trainingJobName, modelName):\n",
    "    \n",
    "    info = sagemaker.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=trainingJobName)\n",
    "    resolvedJobName = info['BestTrainingJob']['TrainingJobName']\n",
    "    print('best training job resolved as :: {}'.format(resolvedJobName))\n",
    "    return createModel(resolvedJobName, modelName)\n",
    "\n",
    "def createModel (trainingJobName, modelName, hyper = False):\n",
    "    \n",
    "    info = sagemaker.describe_training_job(TrainingJobName=trainingJobName)\n",
    "    container = info['AlgorithmSpecification']['TrainingImage']\n",
    "    model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "    print (info)\n",
    "\n",
    "    ## -- print(info)\n",
    "    print(container)\n",
    "    print(model_data)\n",
    "\n",
    "    primary_container = {\n",
    "        'Image': container,\n",
    "        'ModelDataUrl': model_data\n",
    "    }\n",
    "\n",
    "    print('creating model :: {}'.format(modelName))\n",
    "\n",
    "    create_model_response = sagemaker.create_model(\n",
    "        ModelName = modelName,\n",
    "        ExecutionRoleArn = role,\n",
    "        PrimaryContainer = primary_container)\n",
    "\n",
    "    return create_model_response['ModelArn']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a reference to the basic model first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicModelArn = createModel(basicJobName, basicModelName)\n",
    "print('basic model created as : {}'.format(basicModelArn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same for the advanced HPO one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advancedModelArn = createModelFromHpoJob(advancedJobName, advancedModelName)\n",
    "print('advanced model created as : {}'.format(advancedModelArn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've created models from the best jobs, now we create a config referencing both models. \n",
    "\n",
    "The integer values for 'InitialWeightVariant' are whatever you feel gives you the granualarity to express the distribution you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpointConfigName = 'linear-endpoint-config-{}-{}'.format(envname, timestamp)\n",
    "\n",
    "create_endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpointConfigName,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'InstanceType':'ml.m4.xlarge',\n",
    "            'InitialVariantWeight':1,\n",
    "            'InitialInstanceCount':1,\n",
    "            'ModelName':basicModelName,\n",
    "            'InitialVariantWeight':9, \n",
    "            'VariantName':'basicTraffic'}, \n",
    "        {\n",
    "            'InstanceType':'ml.m4.xlarge',\n",
    "            'InitialVariantWeight':1,\n",
    "            'InitialInstanceCount':1,\n",
    "            'ModelName':advancedModelName,\n",
    "            'InitialVariantWeight':1,\n",
    "            'VariantName':'hpoTraffic'\n",
    "        }\n",
    "    ])\n",
    "\n",
    "print(\"Endpoint Config Arn: \" + create_endpoint_config_response['EndpointConfigArn'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an endpoint, using this dual model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "endpointName = 'linear-endpoint-{}-{}'.format(envname, timestamp)\n",
    "\n",
    "create_endpoint_response = sagemaker.create_endpoint(\n",
    "    EndpointName=endpointName,\n",
    "    EndpointConfigName=endpointConfigName)\n",
    "\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sagemaker.describe_endpoint(EndpointName=endpointName)\n",
    "status = resp['EndpointStatus']\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sagemaker.describe_endpoint(EndpointName=endpointName)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp['EndpointArn'])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, at this stage we have an endpoint, balancing between the two models, so let's set up to do some requests\n",
    "\n",
    "First, we need a SageMaker runtime client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define some variables and helper functions, to convert gray scale, and to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgLocalName = 'test_image.jpg'\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot((255-rgb[...,:3]), [0.299, 0.587, 0.114])\n",
    "\n",
    "def predict (body): \n",
    "\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpointName,\n",
    "        Body=body,\n",
    "        ContentType='text/csv',\n",
    "        Accept='application/json'\n",
    "    )\n",
    "\n",
    "    print (response)\n",
    "    # let's look at the body now...\n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    print('----------------------------')\n",
    "    print ('        prediction')\n",
    "    print('----------------------------')\n",
    "\n",
    "    print ('This is a {}'.format(apparel_lookup[int(float(result['predictions'][0]['predicted_label']))]))\n",
    "    print ('We were served by model :: ', response['ResponseMetadata']['HTTPHeaders']['x-amzn-invoked-production-variant'])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download an image and read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imgSrc = 'https://images-na.ssl-images-amazon.com/images/I/81OaXwn1x4L._UX679_.jpg'\n",
    "imgSrc = 'https://images-eu.ssl-images-amazon.com/images/I/31TcgNHsbIL._AC_UL260_SR200,260_.jpg'\n",
    "\n",
    "urllib.request.urlretrieve(imgSrc, imgLocalName)\n",
    "\n",
    "fullimgread = Image.open(imgLocalName)\n",
    "imgread = fullimgread.resize((28, 28))\n",
    "gray = rgb2gray(np.asarray( imgread, dtype=\"int32\" ))\n",
    "# make it a 1d array\n",
    "grayShape = gray.reshape((1,784))\n",
    "\n",
    "# convert to csv for request\n",
    "body = ''\n",
    "for x in np.nditer(grayShape):\n",
    "    body += str((x)) + ','\n",
    "\n",
    "# strip last comma\n",
    "body = body[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're ready to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = predict(body)\n",
    "\n",
    "print (json.dumps(result, indent=4, sort_keys=True))\n",
    "\n",
    "plt.imshow(fullimgread)\n",
    "plt.axis('off')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have a 9 in 10 chance of hitting the basicTraffic model.\n",
    "Let's test that out...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(1, 20):\n",
    "    result = predict(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks plausible, so now let's update the distribution to 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpointName,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            'VariantName': 'basicTraffic',\n",
    "            'DesiredWeight': 5\n",
    "        },\n",
    "        {\n",
    "            'VariantName': 'hpoTraffic',\n",
    "            'DesiredWeight': 5\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now hop over to the SageMaker console, and find your endpoint, and wait for it to finish updating.\n",
    "During the updating process traffic will of course be served by the existing config until the new one is scaled up, health checks passed, and it is ready to serve traffic\n",
    "When it's finished updating, run the same test..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x in range(1, 20):\n",
    "    result = predict(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so that looks pretty even. Now let's go to 100% hpoTraffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sagemaker.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=endpointName,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {\n",
    "            'VariantName': 'basicTraffic',\n",
    "            'DesiredWeight': 0\n",
    "        },\n",
    "        {\n",
    "            'VariantName': 'hpoTraffic',\n",
    "            'DesiredWeight': 10\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to go and look in the SageMaker console to see when it has finished updating, \n",
    "and then run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1, 20):\n",
    "    result = predict(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it, 100% cut over to the newer hpo tuned model.\n",
    "\n",
    "Finally, let's clean up local disk space for good hygiene, as well as the endpoint and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf fashion-mnist.zip\n",
    "!rm -rf fashion-mnist-master\n",
    "!rm -rf test_image.jpg\n",
    "!rm -rf localimage.jpg\n",
    "\n",
    "sagemaker.delete_endpoint(EndpointName=endpointName)\n",
    "sagemaker.delete_endpoint_config(EndpointConfigName=endpointConfigName)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
